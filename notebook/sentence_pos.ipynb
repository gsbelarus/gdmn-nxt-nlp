{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "676ec0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import benepar\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bac732",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789491d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LanguageDetector and assign it a string name\n",
    "@Language.factory(\"language_detector\")\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector(language_detection_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353fc98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x1574c11ff40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"language_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3af25f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3_large to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping models\\benepar_en3_large.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benepar.download('benepar_en3_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c346d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mbenepar_en3\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> benepar.download('benepar_en3')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mmodels/benepar_en3\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'c:\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Golden\\nx\\gdmn-nxt-nlp\\notebook\\sentence_pos.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Golden/nx/gdmn-nxt-nlp/notebook/sentence_pos.ipynb#ch0000005?line=0'>1</a>\u001b[0m nlp\u001b[39m.\u001b[39;49madd_pipe(\u001b[39m\"\u001b[39;49m\u001b[39mbenepar\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mbenepar_en3\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\spacy\\language.py:795\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_factory(factory_name):\n\u001b[0;32m    788\u001b[0m         err \u001b[39m=\u001b[39m Errors\u001b[39m.\u001b[39mE002\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    789\u001b[0m             name\u001b[39m=\u001b[39mfactory_name,\n\u001b[0;32m    790\u001b[0m             opts\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfactory_names),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    793\u001b[0m             lang_code\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang,\n\u001b[0;32m    794\u001b[0m         )\n\u001b[1;32m--> 795\u001b[0m     pipe_component \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_pipe(\n\u001b[0;32m    796\u001b[0m         factory_name,\n\u001b[0;32m    797\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    798\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    799\u001b[0m         raw_config\u001b[39m=\u001b[39;49mraw_config,\n\u001b[0;32m    800\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    801\u001b[0m     )\n\u001b[0;32m    802\u001b[0m pipe_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_pipe_index(before, after, first, last)\n\u001b[0;32m    803\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipe_meta[name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_factory_meta(factory_name)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\spacy\\language.py:674\u001b[0m, in \u001b[0;36mLanguage.create_pipe\u001b[1;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    671\u001b[0m cfg \u001b[39m=\u001b[39m {factory_name: config}\n\u001b[0;32m    672\u001b[0m \u001b[39m# We're calling the internal _fill here to avoid constructing the\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[39m# registered functions twice\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m resolved \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39;49mresolve(cfg, validate\u001b[39m=\u001b[39;49mvalidate)\n\u001b[0;32m    675\u001b[0m filled \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39mfill({\u001b[39m\"\u001b[39m\u001b[39mcfg\u001b[39m\u001b[39m\"\u001b[39m: cfg[factory_name]}, validate\u001b[39m=\u001b[39mvalidate)[\u001b[39m\"\u001b[39m\u001b[39mcfg\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    676\u001b[0m filled \u001b[39m=\u001b[39m Config(filled)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\thinc\\config.py:747\u001b[0m, in \u001b[0;36mregistry.resolve\u001b[1;34m(cls, config, schema, overrides, validate)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    739\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresolve\u001b[39m(\n\u001b[0;32m    740\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    745\u001b[0m     validate: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    746\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m--> 747\u001b[0m     resolved, _ \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_make(\n\u001b[0;32m    748\u001b[0m         config, schema\u001b[39m=\u001b[39;49mschema, overrides\u001b[39m=\u001b[39;49moverrides, validate\u001b[39m=\u001b[39;49mvalidate, resolve\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    749\u001b[0m     )\n\u001b[0;32m    750\u001b[0m     \u001b[39mreturn\u001b[39;00m resolved\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\thinc\\config.py:796\u001b[0m, in \u001b[0;36mregistry._make\u001b[1;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_interpolated:\n\u001b[0;32m    795\u001b[0m     config \u001b[39m=\u001b[39m Config(orig_config)\u001b[39m.\u001b[39minterpolate()\n\u001b[1;32m--> 796\u001b[0m filled, _, resolved \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_fill(\n\u001b[0;32m    797\u001b[0m     config, schema, validate\u001b[39m=\u001b[39;49mvalidate, overrides\u001b[39m=\u001b[39;49moverrides, resolve\u001b[39m=\u001b[39;49mresolve\n\u001b[0;32m    798\u001b[0m )\n\u001b[0;32m    799\u001b[0m filled \u001b[39m=\u001b[39m Config(filled, section_order\u001b[39m=\u001b[39msection_order)\n\u001b[0;32m    800\u001b[0m \u001b[39m# Check that overrides didn't include invalid properties not in config\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\thinc\\config.py:868\u001b[0m, in \u001b[0;36mregistry._fill\u001b[1;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[0;32m    865\u001b[0m     getter \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget(reg_name, func_name)\n\u001b[0;32m    866\u001b[0m     \u001b[39m# We don't want to try/except this and raise our own error\u001b[39;00m\n\u001b[0;32m    867\u001b[0m     \u001b[39m# here, because we want the traceback if the function fails.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m     getter_result \u001b[39m=\u001b[39m getter(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    869\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    870\u001b[0m     \u001b[39m# We're not resolving and calling the function, so replace\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[39m# the getter_result with a Promise class\u001b[39;00m\n\u001b[0;32m    872\u001b[0m     getter_result \u001b[39m=\u001b[39m Promise(\n\u001b[0;32m    873\u001b[0m         registry\u001b[39m=\u001b[39mreg_name, name\u001b[39m=\u001b[39mfunc_name, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[0;32m    874\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\benepar\\integrations\\spacy_plugin.py:176\u001b[0m, in \u001b[0;36mcreate_benepar_component\u001b[1;34m(nlp, name, model, subbatch_max_tokens, disable_tagger)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_benepar_component\u001b[39m(\n\u001b[0;32m    170\u001b[0m     nlp,\n\u001b[0;32m    171\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m     disable_tagger: \u001b[39mbool\u001b[39m,\n\u001b[0;32m    175\u001b[0m ):\n\u001b[1;32m--> 176\u001b[0m     \u001b[39mreturn\u001b[39;00m BeneparComponent(\n\u001b[0;32m    177\u001b[0m         model,\n\u001b[0;32m    178\u001b[0m         subbatch_max_tokens\u001b[39m=\u001b[39;49msubbatch_max_tokens,\n\u001b[0;32m    179\u001b[0m         disable_tagger\u001b[39m=\u001b[39;49mdisable_tagger,\n\u001b[0;32m    180\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\benepar\\integrations\\spacy_plugin.py:116\u001b[0m, in \u001b[0;36mBeneparComponent.__init__\u001b[1;34m(self, name, subbatch_max_tokens, disable_tagger, batch_size)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     97\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     98\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignored\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    102\u001b[0m ):\n\u001b[0;32m    103\u001b[0m     \u001b[39m\"\"\"Load a trained parser model.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39m        batch_size: deprecated and ignored; use subbatch_max_tokens instead\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parser \u001b[39m=\u001b[39m load_trained_model(name)\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m    118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parser\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\benepar\\integrations\\downloader.py:32\u001b[0m, in \u001b[0;36mload_trained_model\u001b[1;34m(model_name_or_path)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_trained_model\u001b[39m(model_name_or_path):\n\u001b[1;32m---> 32\u001b[0m     model_path \u001b[39m=\u001b[39m locate_model(model_name_or_path)\n\u001b[0;32m     33\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparse_chart\u001b[39;00m \u001b[39mimport\u001b[39;00m ChartParser\n\u001b[0;32m     34\u001b[0m     parser \u001b[39m=\u001b[39m ChartParser\u001b[39m.\u001b[39mfrom_trained(model_path)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\benepar\\integrations\\downloader.py:27\u001b[0m, in \u001b[0;36mlocate_model\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     25\u001b[0m         arg \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mnltk.download\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbenepar.download\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(arg)\n\u001b[0;32m     29\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name))\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbenepar_en3\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> benepar.download('benepar_en3')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mmodels/benepar_en3\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'c:\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\torch\\distributions\\distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3d242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'PROPN'), ('is', 'AUX'), ('looking', 'VERB'), ('at', 'ADP'), ('buying', 'VERB'), ('U.K.', 'PROPN'), ('startup', 'NOUN'), ('for', 'ADP'), ('$', 'SYM'), ('1', 'NUM'), ('billion', 'NUM')]\n"
     ]
    }
   ],
   "source": [
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d9508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False 383\n",
      "is be AUX VBZ aux xx True True 0\n",
      "looking look VERB VBG ROOT xxxx True False 0\n",
      "at at ADP IN prep xx True True 0\n",
      "buying buy VERB VBG pcomp xxxx True False 0\n",
      "U.K. U.K. PROPN NNP compound X.X. False False 384\n",
      "startup startup NOUN NN dobj xxxx True False 0\n",
      "for for ADP IN prep xxx True True 0\n",
      "$ $ SYM $ quantmod $ False False 394\n",
      "1 1 NUM CD compound d False False 394\n",
      "billion billion NUM CD pobj xxxx True False 394\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop, token.ent_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c55e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple ORG\n",
      "U.K. U.K. GPE\n",
      "$1 billion $1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, ent.lemma_, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b69438d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple {'Number': 'Sing'}\n",
      "is {'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\n",
      "looking {'Aspect': 'Prog', 'Tense': 'Pres', 'VerbForm': 'Part'}\n",
      "at {}\n",
      "buying {'Aspect': 'Prog', 'Tense': 'Pres', 'VerbForm': 'Part'}\n",
      "U.K. {'Number': 'Sing'}\n",
      "startup {'Number': 'Sing'}\n",
      "for {}\n",
      "$ {}\n",
      "1 {'NumType': 'Card'}\n",
      "billion {'NumType': 'Card'}\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.morph.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048603f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'en', 'score': 0.9999969772583737}\n",
      "Apple is looking at buying U.K. startup for $1 billion\n",
      "{'language': 'en', 'score': 0.9999949386792386}\n",
      "(S (NP (NNP Apple)) (VP (VBZ is) (VP (VBG looking) (PP (IN at) (S (VP (VBG buying) (NP (NNP U.K.) (NN startup)) (PP (IN for) (NP (QP ($ $) (CD 1) (CD billion))))))))))\n"
     ]
    }
   ],
   "source": [
    "# document level language detection. Think of it like average language of the document!\n",
    "print(doc._.language)\n",
    "# sentence level language detection\n",
    "for sent in doc.sents:\n",
    "   print(sent)\n",
    "   print(sent._.language)\n",
    "   print(sent._.parse_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a20fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple nsubj looking {'Number': 'Sing'}\n",
      "is aux looking {'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\n",
      "looking ROOT looking {'Aspect': 'Prog', 'Tense': 'Pres', 'VerbForm': 'Part'}\n",
      "at prep looking {}\n",
      "buying pcomp at {'Aspect': 'Prog', 'Tense': 'Pres', 'VerbForm': 'Part'}\n",
      "U.K. compound startup {'Number': 'Sing'}\n",
      "startup dobj buying {'Number': 'Sing'}\n",
      "for prep buying {}\n",
      "$ quantmod billion {}\n",
      "1 compound billion {'NumType': 'Card'}\n",
      "billion pobj for {'NumType': 'Card'}\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.morph.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a9f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (NNP Apple)) (VP (VBZ is) (VP (VBG looking) (PP (IN at) (S (VP (VBG buying) (NP (NNP U.K.) (NN startup)) (PP (IN for) (NP (QP ($ $) (CD 1) (CD billion))))))))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Apple"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = list(doc.sents)[0]\n",
    "print(sent._.parse_string)\n",
    "sent._.labels\n",
    "list(sent._.children)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e81967",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ru = spacy.load(\"ru_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854845c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x2657d806bf0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_ru.add_pipe(\"language_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d8850",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ru = nlp_ru(\"Покажи все организации из города Минска и Пинска.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf157b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Покажи', 'VERB'), ('все', 'DET'), ('организации', 'NOUN'), ('из', 'ADP'), ('города', 'NOUN'), ('Минска', 'PROPN'), ('и', 'CCONJ'), ('Пинска', 'PROPN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "print([(w.text, w.pos_) for w in doc_ru])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb96ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "[E894] The 'noun_chunks' syntax iterator is not implemented for language 'ru'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Golden\\nx\\gdmn-nxt-nlp\\notebook\\sentence_pos.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Golden/nx/gdmn-nxt-nlp/notebook/sentence_pos.ipynb#ch0000029?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m noun \u001b[39min\u001b[39;00m doc_ru\u001b[39m.\u001b[39mnoun_chunks:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Golden/nx/gdmn-nxt-nlp/notebook/sentence_pos.ipynb#ch0000029?line=1'>2</a>\u001b[0m   \u001b[39mprint\u001b[39m(noun\u001b[39m.\u001b[39mtext, noun\u001b[39m.\u001b[39mroot\u001b[39m.\u001b[39mtext, noun\u001b[39m.\u001b[39mroot\u001b[39m.\u001b[39mdep_)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\spacy\\tokens\\doc.pyx:852\u001b[0m, in \u001b[0;36mnoun_chunks\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: [E894] The 'noun_chunks' syntax iterator is not implemented for language 'ru'."
     ]
    }
   ],
   "source": [
    "for noun in doc_ru.noun_chunks:\n",
    "  print(noun.text, noun.root.text, noun.root.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7643e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent #1\n",
      "############################################################\n",
      "Покажи 0\n",
      "показать VERB VERB Xxxxx True False\n",
      "ROOT 0 Покажи 0\n",
      "[] [организации, .] []\n",
      "{'Aspect': 'Perf', 'Mood': 'Imp', 'Number': 'Sing', 'Person': 'Second', 'VerbForm': 'Fin', 'Voice': 'Act'}\n",
      "############################################################\n",
      "все 1\n",
      "весь DET DET xxx True True\n",
      "det 2 организации 0\n",
      "[0] [все, города] []\n",
      "{'Animacy': 'Inan', 'Case': 'Acc', 'Number': 'Plur'}\n",
      "############################################################\n",
      "организации 2\n",
      "организация NOUN NOUN xxxx True False\n",
      "obj 0 Покажи 0\n",
      "[] [организации, .] []\n",
      "{'Animacy': 'Inan', 'Case': 'Acc', 'Gender': 'Fem', 'Number': 'Plur'}\n",
      "############################################################\n",
      "из 3\n",
      "из ADP ADP xx True True\n",
      "case 4 города 0\n",
      "[2, 0] [из, Минска] []\n",
      "{}\n",
      "############################################################\n",
      "города 4\n",
      "город NOUN NOUN xxxx True False\n",
      "nmod 2 организации 0\n",
      "[0] [все, города] []\n",
      "{'Animacy': 'Inan', 'Case': 'Gen', 'Gender': 'Masc', 'Number': 'Sing'}\n",
      "############################################################\n",
      "Минска 5\n",
      "минск PROPN PROPN Xxxxx True False\n",
      "appos 4 города 0\n",
      "[2, 0] [из, Минска] []\n",
      "{'Animacy': 'Inan', 'Case': 'Gen', 'Gender': 'Masc', 'Number': 'Sing'}\n",
      "############################################################\n",
      "и 6\n",
      "и CCONJ CCONJ x True True\n",
      "cc 7 Пинска 0\n",
      "[5, 4, 2, 0] [и] [Минска]\n",
      "{}\n",
      "############################################################\n",
      "Пинска 7\n",
      "пинск PROPN PROPN Xxxxx True False\n",
      "conj 5 Минска 0\n",
      "[4, 2, 0] [Пинска] [Пинска]\n",
      "{'Animacy': 'Inan', 'Case': 'Gen', 'Gender': 'Masc', 'Number': 'Sing'}\n",
      "############################################################\n",
      ". 8\n",
      ". PUNCT PUNCT . False False\n",
      "punct 0 Покажи 0\n",
      "[] [организации, .] []\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "sent_n = 1\n",
    "for sent in doc_ru.sents:\n",
    "  print('sent #{}'.format(sent_n))\n",
    "  sent_n += 1        \n",
    "  for token in sent:\n",
    "      print('############################################################')\n",
    "      print(token.text, token.i)\n",
    "      print(token.lemma_, token.pos_, token.tag_, token.shape_, token.is_alpha, token.is_stop)\n",
    "      print(token.dep_, token.head.i, token.head.text, token.head.cluster)\n",
    "      print([an.i for an in token.head.ancestors], [child for child in list(token.head.children)], list(token.head.conjuncts))\n",
    "      print(token.morph.to_dict())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96087908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Покажи ROOT Покажи {'Aspect': 'Perf', 'Mood': 'Imp', 'Number': 'Sing', 'Person': 'Second', 'VerbForm': 'Fin', 'Voice': 'Act'}\n",
      "все det организации {'Animacy': 'Inan', 'Case': 'Acc', 'Number': 'Plur'}\n",
      "организации obj Покажи {'Animacy': 'Inan', 'Case': 'Acc', 'Gender': 'Fem', 'Number': 'Plur'}\n",
      "из case города {}\n",
      "города nmod организации {'Animacy': 'Inan', 'Case': 'Gen', 'Gender': 'Masc', 'Number': 'Sing'}\n",
      "Минска appos города {'Animacy': 'Inan', 'Case': 'Gen', 'Gender': 'Masc', 'Number': 'Sing'}\n",
      "и cc Пинска {}\n",
      "Пинска conj Минска {'Animacy': 'Inan', 'Case': 'Gen', 'Gender': 'Masc', 'Number': 'Sing'}\n",
      ". punct Покажи {}\n"
     ]
    }
   ],
   "source": [
    "for token in doc_ru:\n",
    "    print(token.text, token.dep_, token.head.text, token.morph.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9765fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'mk', 'score': 0.7142164920414136}\n",
      "Покажи все организации из города Минска и Пинска. {'language': 'ru', 'score': 0.8571386539265075}\n"
     ]
    }
   ],
   "source": [
    "# document level language detection. Think of it like average language of the document!\n",
    "print(doc_ru._.language)\n",
    "# sentence level language detection\n",
    "for sent in doc_ru.sents:\n",
    "   print(sent, sent._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e6455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Минска минск LOC\n",
      "Пинска пинск LOC\n"
     ]
    }
   ],
   "source": [
    "for sent in doc_ru.sents:\n",
    "  for ent in sent.ents:\n",
    "      print(ent, ent.lemma_, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ru = nlp_ru(\"Василий Петрович Кукушкин купил машину жигули в городе Минске\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a84b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Василий Петрович Кукушкин 0 василий петрович кукушкин PER 0 25\n",
      "Минске 0 минск LOC 55 61\n"
     ]
    }
   ],
   "source": [
    "for ent in doc_ru.ents:\n",
    "    print(ent, ent.ent_id, ent.lemma_, ent.label_, ent.start_char, ent.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607e9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b7a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Василий nsubj купил {'Animacy': 'Anim', 'Case': 'Nom', 'Gender': 'Masc', 'Number': 'Sing'} 0 PER\n",
      "Петрович appos Василий {'Animacy': 'Anim', 'Case': 'Nom', 'Gender': 'Masc', 'Number': 'Sing'} 8 PER\n",
      "Кукушкин appos Василий {'Animacy': 'Anim', 'Case': 'Nom', 'Gender': 'Masc', 'Number': 'Sing'} 17 PER\n",
      "купил ROOT купил {'Aspect': 'Perf', 'Gender': 'Masc', 'Mood': 'Ind', 'Number': 'Sing', 'Tense': 'Past', 'VerbForm': 'Fin', 'Voice': 'Act'} 26 \n",
      "машину obj купил {'Animacy': 'Inan', 'Case': 'Acc', 'Gender': 'Fem', 'Number': 'Sing'} 32 \n",
      "жигули obj купил {'Aspect': 'Perf', 'Mood': 'Ind', 'Number': 'Plur', 'Tense': 'Past', 'VerbForm': 'Fin', 'Voice': 'Act'} 39 \n",
      "в case городе {} 46 \n",
      "городе nmod жигули {'Animacy': 'Inan', 'Case': 'Loc', 'Gender': 'Masc', 'Number': 'Sing'} 48 \n",
      "Минске appos городе {'Animacy': 'Inan', 'Case': 'Loc', 'Gender': 'Masc', 'Number': 'Sing'} 55 LOC\n"
     ]
    }
   ],
   "source": [
    "for token in doc_ru:\n",
    "    print(token.text, token.dep_, token.head.text, token.morph.to_dict(), token._._start, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(\"Apple is looking at buying U.K. startup for $1 billion. Это предложение не на английском. Третье предложение.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed37ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'bg', 'score': 0.9999947070009931}\n",
      "Apple is looking at buying U.K. startup for $1 billion. {'language': 'en', 'score': 0.9999951062958017}\n",
      "Это предложение не на английском. {'language': 'ru', 'score': 0.8571400083231521}\n",
      "Третье предложение. {'language': 'ru', 'score': 0.9999955231308968}\n"
     ]
    }
   ],
   "source": [
    "# document level language detection. Think of it like average language of the document!\n",
    "print(doc2._.language)\n",
    "# sentence level language detection\n",
    "for sent in doc2.sents:\n",
    "   print(sent, sent._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"show me all companies from minsk and pinsk\")\n",
    "#displacy.serve(doc, style=\"dep\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
